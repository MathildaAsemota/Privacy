{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpXLhcPQvao0fMf3hzP88e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MathildaAsemota/Privacy/blob/main/PrivacyPreserving_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split  # For splitting the data\n",
        "from torch.utils.data import SubsetRandomSampler, DataLoader  # For sampling and DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "vuUnDQNHMehC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_new_classes = 2\n",
        "n_epochs = 40\n",
        "new_batch_size = 64\n",
        "learning_rate = 0.0001\n",
        "n_features = 28 * 28\n",
        "n_hidden = 100\n"
      ],
      "metadata": {
        "id": "2xIzcKVCUTJ3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = torchvision.datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor(),\n",
        ")\n",
        "\n",
        "test_data = torchvision.datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "id": "5GYFWsa1UTMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6744dec9-aaf8-44d6-ff6c-7e94aa367e00"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 22.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 651kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 5.61MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.43MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class YourSampler(torch.utils.data.sampler.Sampler):\n",
        "    def __init__(self, mask, data_source):\n",
        "        self.mask = mask\n",
        "        self.data_source = data_source\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter([i.item() for i in torch.nonzero(mask)])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_source)\n",
        "\n",
        "mnist = torchvision.datasets.MNIST(root=\"data\", download=True, transform=transforms.ToTensor())\n",
        "mask = [1 if mnist[i][1] == 0 or mnist[i][1] == 1 else 0 for i in range(len(mnist))]\n",
        "mask = torch.tensor(mask)\n",
        "sampler = YourSampler(mask, mnist)\n",
        "new_data = torch.utils.data.DataLoader(mnist,sampler = sampler, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "gt2Vi6WMUTPG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_images = []\n",
        "all_labels = []\n",
        "\n",
        "for images, labels in new_data:\n",
        "    all_images.append(images)\n",
        "    all_labels.append(labels)\n",
        "\n",
        "all_images = torch.cat(all_images)\n",
        "all_labels = torch.cat(all_labels)\n"
      ],
      "metadata": {
        "id": "Wji16ggKSiF0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_list = []\n",
        "for batch_data, batch_labels in new_data:\n",
        "    batch_data = batch_data.view(batch_data.size(0), -1).numpy()\n",
        "    batch_labels = batch_labels.numpy()\n",
        "    data_list.extend(zip(batch_data, batch_labels))\n",
        "\n",
        "full_data = pd.DataFrame(data_list, columns=['x', 'y'])\n"
      ],
      "metadata": {
        "id": "9gKXOTDDP6oW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "p = 0.7\n",
        "q = 0.9\n",
        "\n",
        "batch_index = 0\n",
        "\n",
        "full_data['Batch'] = -1\n",
        "\n",
        "while(True):\n",
        "    prop = p if (batch_index % 2 == 0) else q\n",
        "    num_class_a_elts = int(batch_size * prop)\n",
        "    num_class_b_elts = batch_size - num_class_a_elts\n",
        "\n",
        "    class_a_unassigned = full_data[(full_data['y'] == 0) & (full_data['Batch'] == -1)]\n",
        "    class_b_unassigned = full_data[(full_data['y'] == 1) & (full_data['Batch'] == -1)]\n",
        "\n",
        "    if class_a_unassigned.shape[0] < num_class_a_elts or \\\n",
        "        class_b_unassigned.shape[0] < num_class_b_elts:\n",
        "        break\n",
        "    else:\n",
        "        full_data.loc[class_a_unassigned.sample(num_class_a_elts).index, 'Batch'] = batch_index\n",
        "        full_data.loc[class_b_unassigned.sample(num_class_b_elts).index, 'Batch'] = batch_index\n",
        "        batch_index += 1"
      ],
      "metadata": {
        "id": "RDvgZ4vzOsba"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = 0.2\n",
        "full_data = full_data[full_data['Batch'] != -1]\n",
        "\n",
        "num_batches = full_data['Batch'].nunique()\n",
        "print(\"Number of batches:\", num_batches)\n",
        "\n",
        "num_test_batches = int(num_batches * test_size)\n",
        "num_train_batches = num_batches - num_test_batches\n",
        "\n",
        "training_data = full_data[full_data['Batch'] < num_train_batches]\n",
        "test_data = full_data[full_data['Batch'] >= num_train_batches]\n",
        "\n",
        "num_batches = training_data['Batch'].nunique()\n",
        "print(\"Number of training batches:\", num_batches)\n",
        "\n",
        "num_t_batches = test_data['Batch'].nunique()\n",
        "print(\"Number of testing batches:\", num_t_batches)\n",
        "\n",
        "input_col = ['x']\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXwgTHyQXU8o",
        "outputId": "6e299f82-70b3-4663-c9c7-d6274cf0cc89"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches: 740\n",
            "Number of training batches: 592\n",
            "Number of testing batches: 148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_batches = training_data.groupby('Batch')\n"
      ],
      "metadata": {
        "id": "kc3-JP5PZG6T"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, n_features: int, n_hidden: int, n_new_classes: int) -> None:\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.h1 = nn.Linear(n_features, n_hidden)\n",
        "        self.h2 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.h3 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.out = nn.Linear(n_hidden, n_new_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.h1(x))\n",
        "        out = torch.relu(self.h2(out))\n",
        "        out = torch.relu(self.h3(out))\n",
        "        out = self.out(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "fkJ3q4SLTSWZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossFunction(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LossFunction, self).__init__()\n",
        "\n",
        "    def forward(self, predictions, mean_l):\n",
        "        mean_p = torch.mean(predictions)\n",
        "        mean_l = torch.tensor(mean_l, dtype=torch.float32)\n",
        "\n",
        "        return torch.square(mean_p - mean_l)"
      ],
      "metadata": {
        "id": "Evia-0wP9e2Y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')\n",
        "model = NeuralNetwork(n_features, n_hidden, n_new_classes).to(device)\n",
        "\n",
        "loss_fn = LossFunction()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "MkaV2h7XTq7B"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = torch.tensor(test_data['x'].tolist(), dtype=torch.float32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCwAhTfmA9Xs",
        "outputId": "6824c280-f47e-4250-f9b0-b1de2cd25a50"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-d4e5fa6900ce>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  input_data = torch.tensor(test_data['x'].tolist(), dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(input_data).detach().numpy()\n",
        "predictions = np.where(predictions < 0.5, 1, 0)\n",
        "predictions = predictions[:, 0]\n",
        "\n",
        "predictions[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1ahg-qgxdoA",
        "outputId": "2f33149e-a0e0-429c-8235-515e6085b70f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_0_data = test_data[test_data['y'] == 1]\n",
        "input_data_class_0 = torch.tensor(class_0_data['x'].tolist(), dtype=torch.float32)\n",
        "predictions_class_0 = model(input_data_class_0).detach().numpy()\n",
        "mu = np.mean(predictions_class_0)\n"
      ],
      "metadata": {
        "id": "kbjclp72T6lE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets = test_data['y']"
      ],
      "metadata": {
        "id": "K1Da8j1JO7jG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "\n",
        "    model.train()\n",
        "    loss_epoch = []\n",
        "\n",
        "    for num, batch in training_data_batches:\n",
        "\n",
        "        prop = batch['y'].value_counts()[0] / batch.shape[0]\n",
        "\n",
        "        model_input = torch.tensor(batch['x'].tolist(), dtype=torch.float32)\n",
        "\n",
        "        output = model(model_input)\n",
        "\n",
        "        loss = loss_fn(output, prop)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss_epoch.append(loss.detach().numpy())\n",
        "\n",
        "    input_data = torch.tensor(test_data['x'].tolist(), dtype=torch.float32)\n",
        "\n",
        "    predictions = model(input_data).detach().numpy()\n",
        "    predictions = np.where(predictions < 0.5, 1, 0)\n",
        "    predictions = predictions[:, 0]\n",
        "\n",
        "    targets = test_data['y']\n",
        "    test_acc = accuracy_score(targets, predictions)\n",
        "\n",
        "    class_0_data = test_data[test_data['y'] == 1]\n",
        "    input_data_class_0 = torch.tensor(class_0_data['x'].tolist(), dtype=torch.float32)\n",
        "    predictions_class_0 = model(input_data_class_0).detach().numpy()\n",
        "    mu = np.mean(predictions_class_0)\n",
        "\n",
        "    class_1_data = test_data[test_data['y'] == 0]\n",
        "    input_data_class_1 = torch.tensor(class_1_data['x'].tolist(), dtype=torch.float32)\n",
        "    predictions_class_1 = model(input_data_class_1).detach().numpy()\n",
        "    nu = np.mean(1 - predictions_class_1)\n",
        "\n",
        "    return loss_epoch, test_acc, mu, nu\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ENF6qQkxyaCp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_history = []\n",
        "test_history = []\n",
        "mu_history = []\n",
        "nu_history = []\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    loss_epoch, test_acc, mu, nu = train(epoch)\n",
        "    print(\"Epoch:\", epoch, \"Loss:\", np.mean(loss_epoch), \"Test Accuracy:\", test_acc, \"mu, nu:\", mu, nu)\n",
        "    loss_history.append(loss_epoch)\n",
        "    test_history.append(test_acc)\n",
        "    mu_history.append(mu)\n",
        "    nu_history.append(nu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mDtN_CTxvKP",
        "outputId": "7fc771d0-10f6-40a0-cd51-d0f23b63edc6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Loss: 0.055508144 Test Accuracy: 0.9736486486486486 mu, nu: 0.26675886 0.043991048\n",
            "Epoch: 2 Loss: 0.004855336 Test Accuracy: 0.9871621621621621 mu, nu: 0.16907014 0.016604215\n",
            "Epoch: 3 Loss: 0.004311548 Test Accuracy: 0.9905405405405405 mu, nu: 0.11838565 0.00975458\n",
            "Epoch: 4 Loss: 0.0038837888 Test Accuracy: 0.9932432432432432 mu, nu: 0.100011386 0.011848529\n",
            "Epoch: 5 Loss: 0.0034370385 Test Accuracy: 0.9945945945945946 mu, nu: 0.09048686 0.016035877\n",
            "Epoch: 6 Loss: 0.0029478294 Test Accuracy: 0.995945945945946 mu, nu: 0.08165192 0.020407155\n",
            "Epoch: 7 Loss: 0.002436045 Test Accuracy: 0.9966216216216216 mu, nu: 0.07294877 0.022174146\n",
            "Epoch: 8 Loss: 0.0019336392 Test Accuracy: 0.9979729729729729 mu, nu: 0.068634726 0.020000666\n",
            "Epoch: 9 Loss: 0.001515352 Test Accuracy: 0.9993243243243243 mu, nu: 0.06404601 0.016249107\n",
            "Epoch: 10 Loss: 0.0011867757 Test Accuracy: 1.0 mu, nu: 0.05820618 0.012832006\n",
            "Epoch: 11 Loss: 0.00094679015 Test Accuracy: 1.0 mu, nu: 0.051276494 0.010529151\n",
            "Epoch: 12 Loss: 0.0007656563 Test Accuracy: 0.9993243243243243 mu, nu: 0.04664647 0.0108284205\n",
            "Epoch: 13 Loss: 0.00062998454 Test Accuracy: 0.9993243243243243 mu, nu: 0.041798487 0.009962331\n",
            "Epoch: 14 Loss: 0.00052288175 Test Accuracy: 0.9993243243243243 mu, nu: 0.03812152 0.009259787\n",
            "Epoch: 15 Loss: 0.0004403927 Test Accuracy: 0.9993243243243243 mu, nu: 0.034726866 0.0082684485\n",
            "Epoch: 16 Loss: 0.00036959673 Test Accuracy: 0.9993243243243243 mu, nu: 0.032864023 0.007569559\n",
            "Epoch: 17 Loss: 0.00031332727 Test Accuracy: 0.9993243243243243 mu, nu: 0.030168733 0.0075953407\n",
            "Epoch: 18 Loss: 0.00026279344 Test Accuracy: 0.9993243243243243 mu, nu: 0.028060203 0.007435973\n",
            "Epoch: 19 Loss: 0.00022476476 Test Accuracy: 0.9993243243243243 mu, nu: 0.025780946 0.007451684\n",
            "Epoch: 20 Loss: 0.00019101205 Test Accuracy: 0.9993243243243243 mu, nu: 0.022041595 0.009701436\n",
            "Epoch: 21 Loss: 0.00016590717 Test Accuracy: 0.9993243243243243 mu, nu: 0.02076148 0.010899905\n",
            "Epoch: 22 Loss: 0.00015407745 Test Accuracy: 0.9993243243243243 mu, nu: 0.02051726 0.013422139\n",
            "Epoch: 23 Loss: 0.00014976625 Test Accuracy: 0.9993243243243243 mu, nu: 0.025904756 0.011123684\n",
            "Epoch: 24 Loss: 0.00017453665 Test Accuracy: 0.9993243243243243 mu, nu: 0.026346166 0.014716784\n",
            "Epoch: 25 Loss: 0.00026394374 Test Accuracy: 1.0 mu, nu: 0.03049091 0.012311103\n",
            "Epoch: 26 Loss: 0.00048128757 Test Accuracy: 1.0 mu, nu: 0.029599061 0.013154671\n",
            "Epoch: 27 Loss: 0.00043042965 Test Accuracy: 1.0 mu, nu: 0.02879436 0.029087843\n",
            "Epoch: 28 Loss: 0.0003151168 Test Accuracy: 1.0 mu, nu: 0.028128726 0.02236512\n",
            "Epoch: 29 Loss: 0.00026860365 Test Accuracy: 1.0 mu, nu: 0.026448386 0.015922235\n",
            "Epoch: 30 Loss: 0.00024974946 Test Accuracy: 1.0 mu, nu: 0.026170041 0.010308178\n",
            "Epoch: 31 Loss: 0.000253764 Test Accuracy: 0.9993243243243243 mu, nu: 0.027690075 0.003909389\n",
            "Epoch: 32 Loss: 0.0002820338 Test Accuracy: 1.0 mu, nu: 0.02973028 0.0015402172\n",
            "Epoch: 33 Loss: 0.0003286128 Test Accuracy: 1.0 mu, nu: 0.029752377 0.0028988416\n",
            "Epoch: 34 Loss: 0.00043048232 Test Accuracy: 1.0 mu, nu: 0.030193148 0.013561877\n",
            "Epoch: 35 Loss: 0.00056385575 Test Accuracy: 1.0 mu, nu: 0.030618813 0.030618208\n",
            "Epoch: 36 Loss: 0.00064197957 Test Accuracy: 1.0 mu, nu: 0.03228165 0.032905404\n",
            "Epoch: 37 Loss: 0.00055313285 Test Accuracy: 1.0 mu, nu: 0.032062132 0.032473803\n",
            "Epoch: 38 Loss: 0.0004390901 Test Accuracy: 1.0 mu, nu: 0.028606331 0.03391265\n",
            "Epoch: 39 Loss: 0.00035356332 Test Accuracy: 1.0 mu, nu: 0.025743851 0.035379685\n",
            "Epoch: 40 Loss: 0.00028312803 Test Accuracy: 1.0 mu, nu: 0.024024924 0.035738844\n"
          ]
        }
      ]
    }
  ]
}